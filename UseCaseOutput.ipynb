{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e70665f5",
   "metadata": {},
   "source": [
    "## The final model:\n",
    "\n",
    "### Here we will gather all our modules and execute our algorithm\n",
    "\n",
    "we first start with importing our modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "047b5519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import import_ipynb\n",
    "from UseCaseFunc import preprocess, train_and_load_ner_model, extract_entities_as_dict, train_sentiment_model, predict_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11069823",
   "metadata": {},
   "source": [
    "### let's do some training first\n",
    "\n",
    "#### We start with training and then loading our NER model for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7af33a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 1335.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4m[i] Saving to output directory: .\u001b[0m\n",
      "\u001b[38;5;4m[i] Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m[+] Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4m[i] Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4m[i] Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     45.21    0.00    0.00    0.00    0.00\n",
      " 33     200        191.76   1222.34  100.00  100.00  100.00    1.00\n",
      " 75     400          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "126     600          0.82      0.27  100.00  100.00  100.00    1.00\n",
      "191     800          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "266    1000          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "366    1200          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "466    1400          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "613    1600          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "813    1800          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "\u001b[38;5;2m[+] Saved pipeline to output directory\u001b[0m\n",
      "model-last\n"
     ]
    }
   ],
   "source": [
    "ner_model = train_and_load_ner_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dea625",
   "metadata": {},
   "source": [
    "This is a test to be sure our model is loaded and fast in terms of Name Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "122f3f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'poisoning': 'HEALTH_ISSUE', 'several': 'LEGAL_ISSUES', 'regulations.': 'LEGAL_ISSUES', 'legal action': 'LEGAL_ISSUES'}\n"
     ]
    }
   ],
   "source": [
    "input_text= \"Our experience at FoodieLand was a nightmare. Not only did we endure a severe case of food poisoning, but we also discovered that the restaurant had violated several health and safety regulations. We're considering legal action due to the extent of our suffering and the blatant negligence on their part.\"\n",
    "entities = extract_entities_as_dict (input_text, ner_model)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8abd6f",
   "metadata": {},
   "source": [
    "#### Now let's train the sentiment analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe9d7312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 107s 4s/step - loss: 0.6940 - accuracy: 0.4938 - precision: 0.4894 - recall: 0.5227\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 99s 4s/step - loss: 0.6936 - accuracy: 0.5025 - precision: 0.4975 - recall: 0.5126\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 99s 4s/step - loss: 0.6902 - accuracy: 0.5250 - precision: 0.5172 - recall: 0.6061\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 100s 4s/step - loss: 0.6883 - accuracy: 0.5487 - precision: 0.5368 - recall: 0.6439\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 99s 4s/step - loss: 0.6884 - accuracy: 0.5562 - precision: 0.5398 - recall: 0.7020\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 99s 4s/step - loss: 0.6867 - accuracy: 0.5763 - precision: 0.5593 - recall: 0.6793\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 617s 26s/step - loss: 0.6871 - accuracy: 0.5875 - precision: 0.5690 - recall: 0.6869\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 90s 4s/step - loss: 0.6841 - accuracy: 0.5987 - precision: 0.5895 - recall: 0.6237\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 90s 4s/step - loss: 0.6824 - accuracy: 0.6112 - precision: 0.6005 - recall: 0.6414\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 91s 4s/step - loss: 0.6805 - accuracy: 0.6200 - precision: 0.6156 - recall: 0.6187\n",
      "7/7 [==============================] - 23s 3s/step - loss: 0.6800 - accuracy: 0.7000 - precision: 0.7444 - recall: 0.6442\n",
      "7/7 [==============================] - 23s 3s/step\n"
     ]
    }
   ],
   "source": [
    "trained_data = train_sentiment_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b4fb35",
   "metadata": {},
   "source": [
    "Let's test it and make sure it's working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9549d8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 185ms/step\n"
     ]
    }
   ],
   "source": [
    "preprocessed = preprocess(input_text)\n",
    "preprocessed = [preprocessed]\n",
    "#predict the sentiment\n",
    "predictions = predict_sentiment(preprocessed, trained_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f6e35ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4f38c9",
   "metadata": {},
   "source": [
    "### Input function that will help us take the feedback and the list of keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cda57c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Client_Input():\n",
    "    # Get input sentence from user\n",
    "    sentence = input(\"Feedback to process\")\n",
    "\n",
    "    # Get input list of words from user\n",
    "    word_list = input(\"Insert your SEO keywords, use a comma between each word \").split(',')\n",
    "\n",
    "    # Saving the sentence in a variable\n",
    "    processed_sentence = sentence.strip()\n",
    "\n",
    "    # Saving the list of words in a variable\n",
    "    processed_word_list = [word.strip() for word in word_list]\n",
    "\n",
    "    return processed_sentence, processed_word_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e187bea",
   "metadata": {},
   "source": [
    "### function to analyzing feedback extracted entities and see if the input is about one of our sensitive topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df53d81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feedback_entities(tagged_words):\n",
    "    # Initialize flags for each entity type\n",
    "    has_health_issue = False\n",
    "    has_legal_issue = False\n",
    "    has_weather_issue = False\n",
    "\n",
    "    # Check if the feedback mentions entities related to HEALTH_ISSUE\n",
    "    if any(tag == 'HEALTH_ISSUE' for tag in tagged_words.values()):\n",
    "        has_health_issue = True\n",
    "\n",
    "    # Check if the feedback mentions entities related to LEGAL_ISSUES\n",
    "    if any(tag == 'LEGAL_ISSUES' for tag in tagged_words.values()):\n",
    "        has_legal_issue = True\n",
    "\n",
    "    # Check if the feedback mentions entities related to WEATHER\n",
    "    if any(tag == 'WEATHER' for tag in tagged_words.values()):\n",
    "        has_weather_issue = True\n",
    "\n",
    "    return {\n",
    "        'HEALTH_ISSUE': has_health_issue,\n",
    "        'LEGAL_ISSUES': has_legal_issue,\n",
    "        'WEATHER': has_weather_issue\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2e0e85",
   "metadata": {},
   "source": [
    "### Answer Generation function which will give us our desired out put"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "61b1bd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(feedback, sentiment, keywords, tagwords):\n",
    "    \n",
    "    #Mapping Keywords in order to use them adequately in our responses templates\n",
    "    keyword_mapping = {\n",
    "        'Restaurant_name': keywords[0],\n",
    "        'Country': keywords[1],\n",
    "        'Restaurant_country': keywords[2],\n",
    "        'City': keywords[3],\n",
    "        'Street': keywords[4],\n",
    "        'Monument': keywords[5],\n",
    "        'Food_type': keywords[6],\n",
    "        'Terasse': keywords[7],\n",
    "        'Email': keywords[8],\n",
    "        'Negative_feedback': keywords [9],\n",
    "        'Special_plate': keywords[10],\n",
    "        'Restaurant_type': keywords[11]      \n",
    "    }\n",
    "\n",
    "    if sentiment == 1:\n",
    "        # List of positive responses\n",
    "        positive_responses_template = [\n",
    "          \"Thank you so much for your kind words! We're thrilled to hear that you enjoyed your dining experience. At {Restaurant_name}, we're committed to delivering exceptional service and delicious dishes. Your satisfaction is our top priority! If you ever crave our signature {Special_plate}, we'd love to welcome you back for another delightful experience. Don't forget to check our website for the latest {Country} menu updates and exclusive promotions. Cheers to more memorable moments at our {Restaurant_country} ! #BestRestaurant #FoodieHeaven #DiningDelight\",\n",
    "          \"We're overjoyed to receive your positive feedback! It warms our hearts to know that you had a fantastic time at {Restaurant_name} the nearby restaurant to {Street}. Our team works hard to create a welcoming atmosphere and serve mouthwatering dishes. Planning your next visit? Explore our website for exciting events, seasonal specials, and exclusive offers. Whether you're a local or visiting from afar, we aim to make every experience unforgettable. Rendez-vous near the {Monument} #FoodieParadise #CustomerLove #GourmetExperience\",\n",
    "          \"We're delighted to hear about your wonderful experience at {Restaurant_name}! Your positive feedback fuels our passion for delivering exceptional dining moments. Looks like you really enjoyed our {Restaurant_country} menu. Your satisfaction is our inspiration, and we can't wait to welcome you back for another round of culinary excellence in {City}. Thank you for choosing {Restaurant_name}!  #GourmetCuisine #CustomerAppreciation #DiningElegance\"\n",
    "        ]\n",
    "        return random.choice(positive_responses_template).format(**keyword_mapping)\n",
    "    \n",
    "\n",
    "    elif sentiment == 0 and tagwords['WEATHER']:\n",
    "        # Specific response for negative feedback due to bad weather\n",
    "        weather_response = \"We apologize for any inconvenience caused by the weather during your visit. We understand the impact of bad weather on the dining experience. Your feedback is valuable as {Restaurant_name} the {Restaurant_country} relies on {Negative_feedback}, and we will explore measures to address weather-related issues. If you have any specific suggestions, please share them with us. Thank you for bringing this to our attention.\"\n",
    "\n",
    "        return weather_response.format(**keyword_mapping)\n",
    "\n",
    "\n",
    "    elif sentiment == 0 and tagwords['HEALTH_ISSUE']:\n",
    "        # Specific response for negative feedback with a medical issue\n",
    "        medical_issue_response = \"We're genuinely sorry to hear about your experience, and we're concerned to learn about any medical issues. Your health and well-being is the top priority of {Restaurant_name}. Please contact us directly at {Email} so we can address this matter promptly and take appropriate actions. Thank you for bringing this to our attention.\"\n",
    "\n",
    "        return medical_issue_response.format(**keyword_mapping)\n",
    "    \n",
    "    elif sentiment == 0 and tagwords['LEGAL_ISSUES']:\n",
    "        # Specific response for negative feedback with violence or assault mention\n",
    "        violence_response = \"We are deeply concerned to hear about your experience, and we take matters of safety very seriously. Your well-being is our top priority. Please contact us immediately at {Email} to provide more details, and we will investigate this matter urgently. Thank you for bringing this to our attention. We also would like to invite you for a cocktail as our treat you.\"\n",
    "\n",
    "        return violence_response.format(**keyword_mapping)\n",
    "\n",
    "    elif sentiment == 0:\n",
    "        # List of general negative responses\n",
    "        negative_responses = [\n",
    "            \"We're sorry to hear that you had a less than satisfactory experience. Please let us know how we can improve so that we can achieve the best customer experience at {Restaurant_name} as {Restaurant_country} in {City}. Your {Negative_feedback} will help us improve.\",\n",
    "            \"Thank you for bringing this to our attention. We apologize for any inconvenience caused.\",\n",
    "            \"Your feedback is important to us. We apologize if the experience at our {Restaurant_country} fell short of expectations.\",\n",
    "            \"At {Restaurant_name}, we strive for excellence in customer experience. We apologize for any inconvenience you may have faced.\"\n",
    "        ]\n",
    "        return random.choice(negative_responses).format(**keyword_mapping)\n",
    "\n",
    "    else:\n",
    "        return f\"Thank you for your feedback. We appreciate your input. Keywords: {', '.join(tokens)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "313a7597",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedback to processI got very sick after eating sushi at shushimi. I do not recommend it.\n",
      "Insert your SEO keywords, use a comma between each word sushimi, japanese, japanese restaurant, london, picadelli street, london eye, halal, terasse, sushimi@gmail.com, constructive criticism, noodles , restaurant\n",
      "1/1 [==============================] - 0s 200ms/step\n",
      "[[0]]\n",
      "We're genuinely sorry to hear about your experience, and we're concerned to learn about any medical issues. Your health and well-being is the top priority of sushimi. Please contact us directly at sushimi@gmail.com so we can address this matter promptly and take appropriate actions. Thank you for bringing this to our attention.\n"
     ]
    }
   ],
   "source": [
    "# Client Input and SEO keywords:\n",
    "result_sentence, keywords = Client_Input()\n",
    "#Extracting sensitive entities for more customization of response\n",
    "result_dict = extract_entities_as_dict(result_sentence, ner_model)\n",
    "#Analyzing our sensitive entities to see if they exist in the input\n",
    "result = analyze_feedback_entities(result_dict)\n",
    "#Preparing the sentence for sent analysis (hard coded sorry :c)\n",
    "preprocessed = preprocess(result_sentence)\n",
    "preprocessed = [preprocessed]\n",
    "#predict the sentiment\n",
    "predictions = predict_sentiment(preprocessed, trained_data)\n",
    "print(predictions)\n",
    "#Generate the response\n",
    "response = generate_response(result_sentence, predictions[0][0], keywords, result)\n",
    "#output\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
